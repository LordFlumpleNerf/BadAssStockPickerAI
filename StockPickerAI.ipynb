{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnpxeOwXsmivzy7vD24KRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LordFlumpleNerf/BadAssStockPickerAI/blob/main/StockPickerAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQD1NlKW4PUq"
      },
      "outputs": [],
      "source": [
        "### THIS BLOCK IS A CATCH-ALL FOR SHIT TO IMPORT AND SETUP\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import alpha_vantage\n",
        "import requests\n",
        "import json\n",
        "#import nasdaq_data_link\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import models, layers, callbacks\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Dropout\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime, date, timedelta\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "import pytz\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "#pd.set_option('display.width', 300000)\n",
        "pd.set_option('display.colheader_justify', 'center')\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM_zhlHJL3QR"
      },
      "outputs": [],
      "source": [
        "av_api_key = \"API_KEY_from_Alpha_Vantage\"\n",
        "alpha_vantage_url = 'https://www.alphavantage.co/query'\n",
        "\n",
        "sample_size = 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqOjR_y1sWAj"
      },
      "outputs": [],
      "source": [
        "#drawing our tickers from iShares ICLN fund or iShares Core Small Cap ETF\n",
        "### ADDED COLUMN IN EACH UNDER 'usecols' FOR SECTOR\n",
        "\n",
        "#stocklist = pd.read_excel('drive/My Drive/iShares-Global-Clean-Energy-ETF_fund.xlsx', sheet_name='Holdings', usecols='A,D,E,L', skiprows=7)\n",
        "#stocklist = pd.read_excel('drive/My Drive/iShares-Core-SP-Small-Cap-ETF_fund.xlsx', sheet_name='Holdings', usecols='A,D,E,L', skiprows=7)\n",
        "#stocklist = pd.read_excel('drive/My Drive/iShares-Core-SP-Total-US-Stock-Market-ETF_fund.xlsx', sheet_name='Holdings', usecols='A,C,D,K', skiprows=7)\n",
        "\n",
        "mid_caps = pd.read_excel('drive/My Drive/iShares-Core-SP-Mid-Cap-ETF_fund.xlsx', sheet_name='Holdings', usecols='A,D,E,L', skiprows=7)\n",
        "large_caps = pd.read_excel('drive/My Drive/iShares-Core-SP-500-ETF_fund.xlsx', sheet_name='Holdings', usecols='A,C,D,K', skiprows=7)\n",
        "\n",
        "stocklist = pd.concat([large_caps, mid_caps])\n",
        "stocklist.reset_index(drop=True)\n",
        "\n",
        "del mid_caps\n",
        "del large_caps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2cUfLw04PYm"
      },
      "outputs": [],
      "source": [
        "#Creating initial ticker list from ICLN or Small-Cap tickers\n",
        "### CHANGING TO A DATAFRAME TO ADD SECTOR\n",
        "stocklist = stocklist[(stocklist['Asset Class'] == 'Equity')]\n",
        "stocklist = stocklist[(stocklist['Exchange'] == 'NASDAQ') | (stocklist['Exchange'] == 'New York Stock Exchange Inc.')]\n",
        "print(stocklist.head())\n",
        "print(stocklist.tail())\n",
        "stocklist = stocklist[['Ticker', 'Sector']].sample(sample_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0ha3g4q4Par"
      },
      "outputs": [],
      "source": [
        "### one-hot encoding for 'sector'\n",
        "ticker = stocklist['Ticker']\n",
        "sector = pd.get_dummies(stocklist['Sector'])\n",
        "stocklist = pd.concat([ticker, sector], axis=1)\n",
        "stocklist[sector.columns] = stocklist[sector.columns].astype(float)\n",
        "stocklist.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#print(stocklist.head())\n",
        "del sector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TEF5d1xT0s9"
      },
      "outputs": [],
      "source": [
        "def get_indices(index_ticker, period):\n",
        "    # for getting index dataframes for each important index\n",
        "    ticker = yf.Ticker(index_ticker)\n",
        "\n",
        "    #initializing the frame\n",
        "    df = pd.DataFrame(ticker.history(period=period))\n",
        "    #df.reset_index(drop=True, inplace=True)\n",
        "    df.drop(columns=['Dividends', 'Stock Splits'], axis=1, inplace=True)\n",
        "    if index_ticker == '^VIX':\n",
        "        df.drop(['Volume'], axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXpFNurpCGGV"
      },
      "outputs": [],
      "source": [
        "def get_cpi(start_date):\n",
        "    # alpha_vantage getting cpi data\n",
        "    params = {\n",
        "        'function': 'CPI',\n",
        "        'interval': 'monthly',\n",
        "        'apikey': av_api_key\n",
        "        }\n",
        "\n",
        "    response = requests.get(alpha_vantage_url, params)\n",
        "    data = response.json()\n",
        "    cpi = pd.DataFrame(data['data'])\n",
        "    cpi = cpi.iloc[::-1]\n",
        "    cpi.set_index('date', inplace=True)\n",
        "    cpi = cpi[start_date:]\n",
        "    cpi.reset_index(inplace=True)\n",
        "    starting_cpi = cpi['value'].loc[0]\n",
        "\n",
        "    return cpi, starting_cpi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZbAL0ObeItJ"
      },
      "outputs": [],
      "source": [
        "def get_ffr(start_date):\n",
        "    # alpha_vantage getting federal funds rate\n",
        "    params = {\n",
        "        'function': 'FEDERAL_FUNDS_RATE',\n",
        "        'interval': 'daily',\n",
        "        'apikey': av_api_key\n",
        "        }\n",
        "\n",
        "    response = requests.get(alpha_vantage_url, params)\n",
        "    data = response.json()\n",
        "    ffr = pd.DataFrame(data['data'])\n",
        "    ffr = ffr.iloc[::-1]\n",
        "    ffr.set_index('date', inplace=True)\n",
        "    ffr = ffr[start_date:]\n",
        "    ffr.reset_index(inplace=True)\n",
        "\n",
        "    return ffr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVVTreywMC5G"
      },
      "outputs": [],
      "source": [
        "def get_earnings(ticker):\n",
        "    # We are going to alpha_vantage for this\n",
        "    params = {\n",
        "        'function': 'EARNINGS',\n",
        "        'symbol': ticker,\n",
        "        'outputsize': 'full',\n",
        "        'datatype': 'json',\n",
        "        'apikey': av_api_key\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(alpha_vantage_url, params)\n",
        "        data = response.json()\n",
        "        quarterly_earnings = pd.DataFrame(data['quarterlyEarnings'])\n",
        "        quarterly_earnings['fiscalDateEnding'] = pd.to_datetime(quarterly_earnings['fiscalDateEnding'])\n",
        "\n",
        "        return quarterly_earnings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An exception occurred: \", e)\n",
        "\n",
        "        return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb3RlAwa4PcP"
      },
      "outputs": [],
      "source": [
        "def create_frame(stock_ticker, timeframe, future_offset, index_length, merged_economic_data, period='max'):\n",
        "    #creating and forming the master frame, converting data to float (except for ticker which will be done later)\n",
        "    #data taken from yahoo finance\n",
        "    ### TAKING OUT DATE COLUMNS AS THEY COULD CORRUPT RESULTS SINCE DATE FOR PREDICTION DATA WILL ALWAYS BE CURRENT\n",
        "\n",
        "    # Create the initial dataframe\n",
        "    try:\n",
        "        ticker = yf.Ticker(stock_ticker)\n",
        "\n",
        "        #initializing the frame\n",
        "        df = pd.DataFrame(ticker.history(period=period))\n",
        "    except ValueError as e:\n",
        "        if str(e) == f\"{stock_ticker}: No data found, symbol may be delisted\":\n",
        "            print(f\"Symbol {symbol} is not available.\")\n",
        "        else:\n",
        "          raise e\n",
        "\n",
        "    if df.empty:\n",
        "        raise DataError(f\"No data available for {ticker}.\")\n",
        "    elif len(df) < index_length:\n",
        "        raise DataError(f\"Insufficient data available for {ticker}.\")\n",
        "    elif df.isna().values.any():\n",
        "        raise DataError(f\"Null values found for {ticker}.\")\n",
        "\n",
        "    # Now create the earnings dataframe and prepare it\n",
        "    try:\n",
        "        df_earnings = get_earnings(stock_ticker)\n",
        "        df_earnings = df_earnings.iloc[::-1]\n",
        "        df_earnings.reset_index(drop=True, inplace=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    df_earnings['reportedEPS'] = df_earnings['reportedEPS'].replace('None', 0.0)\n",
        "    df_earnings['estimatedEPS'] = df_earnings['estimatedEPS'].replace('None', 0.0)\n",
        "    df_earnings['surprise'] = df_earnings['surprise'].replace('None', 0.0)\n",
        "    df_earnings['surprisePercentage'] = df_earnings['surprisePercentage'].replace('None', 0.0)\n",
        "\n",
        "    # Now combine earnings and also cpi and federal funds rate data\n",
        "\n",
        "    #initialize earnings to 0.0\n",
        "    df['reportedEPS'] = 0.0\n",
        "    df['estimatedEPS'] = 0.0\n",
        "    df['surprise'] = 0.0\n",
        "    df['surprisePercentage'] = 0.0\n",
        "\n",
        "    #set up a date column for our loops\n",
        "    df['date'] = pd.to_datetime(df.index.date)\n",
        "    df['date'] = df['date'].dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    #earnings loop\n",
        "    for i in range(len(df)):\n",
        "        target_date = df.loc[i, 'date']\n",
        "\n",
        "        #add in earnings\n",
        "        for j in range(len(df_earnings)):\n",
        "            reported_date = df_earnings.loc[j, 'reportedDate']\n",
        "            if reported_date == target_date:\n",
        "                df.loc[i, 'reportedEPS'] = df_earnings.loc[j, 'reportedEPS']\n",
        "                df.loc[i, 'estimatedEPS'] = df_earnings.loc[j, 'estimatedEPS']\n",
        "                df.loc[i, 'surprise'] = df_earnings.loc[j, 'surprise']\n",
        "                df.loc[i, 'surprisePercentage'] = df_earnings.loc[j, 'surprisePercentage']\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    #merge in economics\n",
        "    df = pd.merge(df, merged_economic_data, on='date', how='left')\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    df = df.drop('date', axis=1)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    #converting some things to type float and calculating \"future close\" which is a column for price \"future_offset\" days in the future\n",
        "    df['Dividends'] = df['Dividends'].astype('float')\n",
        "    df['Stock Splits'] = df['Stock Splits'].astype('float')\n",
        "    df['Volume'] = df['Volume'].astype('float')\n",
        "    df['Future Close'] = df['Close'].shift(-(future_offset))\n",
        "\n",
        "    #getting most recent close\n",
        "    most_recent_close = df['Close'].iloc[-1]\n",
        "\n",
        "    del df_earnings\n",
        "\n",
        "    return df, most_recent_close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYfcbfzEH7wK"
      },
      "outputs": [],
      "source": [
        "def create_sets(frame, timeframe, future_offset, input_scaler, number_of_selections=20):\n",
        "    #Creating the X and ys for each individual stock\n",
        "    # THIS FUNCTION RETURNS A LIST CONTAINING FRAMES FOR ONE STOCK\n",
        "\n",
        "    #initializing train, test, and validation\n",
        "    train_X = []\n",
        "    train_y = []\n",
        "    validation_X = []\n",
        "    validation_y = []\n",
        "    prediction_set = []\n",
        "\n",
        "    #creating the prediction set and adding one_hot\n",
        "    predict_set = frame.iloc[-timeframe:].drop(['Future Close'], axis=1)\n",
        "    predict_set.reset_index(drop=True, inplace=True)\n",
        "    predict_set = input_scaler.transform(predict_set)\n",
        "    prediction_set.append(predict_set)\n",
        "\n",
        "    #dropping future_offset as it contains unpredictable values for future_close and move_level\n",
        "    frame = frame.iloc[:-future_offset]\n",
        "\n",
        "    #creating a sample size to be divided among sets\n",
        "    #number_of_selections = int((len(frame)) / ((timeframe) / 3))\n",
        "\n",
        "    training_size = int(number_of_selections * .8)\n",
        "    training_selections = random.sample(range(len(frame)-timeframe), training_size)\n",
        "    for i in training_selections:\n",
        "        X_frame = frame.iloc[i:i+timeframe].drop(['Future Close'], axis=1)\n",
        "        X_frame.reset_index(drop=True, inplace=True)\n",
        "        X_frame = input_scaler.transform(X_frame)\n",
        "        y = frame.iloc[i+timeframe]['Future Close']\n",
        "        train_X.append(X_frame)\n",
        "        train_y.append(y.item())\n",
        "\n",
        "    validation_size = int(number_of_selections * .2)\n",
        "    validation_selections = random.sample(range(len(frame)-timeframe), validation_size)\n",
        "    for i in validation_selections:\n",
        "        X_frame = frame.iloc[i:i+timeframe].drop(['Future Close'], axis=1)\n",
        "        X_frame.reset_index(drop=True, inplace=True)\n",
        "        X_frame = input_scaler.transform(X_frame)\n",
        "        y = frame.iloc[i+timeframe]['Future Close']\n",
        "        validation_X.append(X_frame)\n",
        "        validation_y.append(y.item())\n",
        "\n",
        "    return train_X, train_y, validation_X, validation_y, prediction_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNnK5zhZxDPY"
      },
      "outputs": [],
      "source": [
        "def prepare_data(combined_frames, timeframe, future_offset, input_scaler, number_of_selections=20): #removed scaler\n",
        "    # The catch all function to run the functions that organize the data\n",
        "    # will return list of dataframes that are scaled\n",
        "    train_data = []\n",
        "    train_labels = []\n",
        "    validation_data = []\n",
        "    validation_labels = []\n",
        "    predict_data = []\n",
        "    for each in combined_frames:\n",
        "        #scaled_frame = scale_data(combined_frames[each])#, scaler)\n",
        "        t_X, t_y, v_X, v_y, p_s = create_sets(combined_frames[each], timeframe, future_offset, input_scaler, number_of_selections)\n",
        "        train_data.extend(t_X)\n",
        "        train_labels.extend(t_y)\n",
        "        validation_data.extend(v_X)\n",
        "        validation_labels.extend(v_y)\n",
        "        predict_data.extend(p_s)\n",
        "\n",
        "    return train_data, train_labels, validation_data, validation_labels, predict_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfweQD9Q90eO"
      },
      "outputs": [],
      "source": [
        "def combine_with_indexes(index_dict, frame_dict):\n",
        "        #redoing the dictionary of frames to add stock indexes for the same date ranges\n",
        "    new_frame_dict = {\n",
        "        ticker: pd.concat([frame, pd.concat(index_dict.values(), axis=1)], axis=1)\n",
        "        for ticker, frame in frame_dict.items()\n",
        "    }\n",
        "\n",
        "    return new_frame_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ePYW32f5rnJ"
      },
      "outputs": [],
      "source": [
        "class DataError(Exception):\n",
        "    #creating an exception class to ensure adequate data per stock in list\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMCJ4-0txC7p"
      },
      "outputs": [],
      "source": [
        "def array_stacker(dataframe_list):\n",
        "    #converting input dataframes into appropriate tensor form\n",
        "    return tf.stack([dataframe_list])\n",
        "\n",
        "def label_stacker(label_list):\n",
        "    #converting labels into appropriate tensor form\n",
        "    return tf.stack([label_list[each] for each in range(len(label_list))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV-Z17U1vMvV"
      },
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IM-lAnJ4PeW"
      },
      "outputs": [],
      "source": [
        "indices = {\n",
        "    'SPX':'^GSPC',\n",
        "    'Dow':'^DJI',\n",
        "    'Nasdaq':'^IXIC',\n",
        "    'Russell':'^RUT',\n",
        "    'NYSE':'^NYA',\n",
        "    'AMEX':'^XAX',\n",
        "    'XLF':'XLF',\n",
        "    'XLE':'XLE',\n",
        "    'XLU':'XLU',\n",
        "    'XLB':'XLB',\n",
        "    'XLI':'XLI',\n",
        "    'XLY':'XLY',\n",
        "    'XLV':'XLV',\n",
        "    'SMH':'SMH',\n",
        "    'XLP':'XLP',\n",
        "    'XTL':'XTL',\n",
        "    'IYR':'IYR',\n",
        "    'VIX':'^VIX'\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKgtdirT4Pgz"
      },
      "outputs": [],
      "source": [
        "timeframe = 20\n",
        "period = '5y'\n",
        "future_offset = 1\n",
        "number_of_selections = 100\n",
        "\n",
        "# Create dictionary of dataframes for indices\n",
        "index_dict = {}\n",
        "start_date = str(0)\n",
        "for key, value in indices.items():\n",
        "    df = get_indices(value, period)\n",
        "    if key == 'SPX':\n",
        "        start_date = df.index[0].strftime('%Y-%m-%d')\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    index_dict[key] = df\n",
        "\n",
        "index_length = len(index_dict['SPX'])-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wr8E3w44PiN"
      },
      "outputs": [],
      "source": [
        "# let's define a few global variable dataframes to reuse\n",
        "ffr = get_ffr(start_date)\n",
        "cpi, starting_cpi = get_cpi(start_date)\n",
        "cpi.loc[cpi['value'].isna(), 'value'] = starting_cpi\n",
        "\n",
        "merged_economic_data = pd.merge(ffr, cpi, on='date', how='left')\n",
        "merged_economic_data['value_y'].fillna(method='ffill', inplace=True)\n",
        "merged_economic_data.loc[merged_economic_data['value_y'].isna(), 'value_y'] = starting_cpi\n",
        "merged_economic_data.rename(columns={'value_x': 'ffr', 'value_y': 'cpi'}, inplace=True)\n",
        "\n",
        "x = 0\n",
        "frame_dict = {}\n",
        "close_dict = {}\n",
        "\n",
        "sector_columns = [stocklist.columns[1:].tolist()]\n",
        "\n",
        "for idx, row in stocklist.iterrows():\n",
        "    time.sleep(15)\n",
        "    ticker = row['Ticker']\n",
        "    if x < 100:\n",
        "        print(ticker, end=\", \")\n",
        "        x += 1\n",
        "    else:\n",
        "        print(ticker)\n",
        "        print()\n",
        "        x = 0\n",
        "        time.sleep(600)\n",
        "\n",
        "    try:\n",
        "        frame, most_recent_close = create_frame(ticker, timeframe, future_offset, index_length, merged_economic_data, period)\n",
        "        for each in sector_columns:\n",
        "            frame[each] = row[each]\n",
        "        frame_dict[ticker] = frame\n",
        "        close_dict[ticker] = most_recent_close\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error for {ticker}: {e}\")\n",
        "        pass\n",
        "\n",
        "del stocklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym-YI1Po9LXo"
      },
      "outputs": [],
      "source": [
        "combined_frames = combine_with_indexes(index_dict, frame_dict)\n",
        "\n",
        "del index_dict\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scale_input_frame = pd.concat(combined_frames)\n",
        "scale_input_frame = scale_input_frame.drop(columns=['Future Close'])\n",
        "input_scaler = scaler.fit(scale_input_frame)\n",
        "\n",
        "train_data, train_labels, validation_data, validation_labels, predict_data = prepare_data(combined_frames, timeframe, future_offset, input_scaler, number_of_selections)\n",
        "del combined_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx5yfjeLlkMH"
      },
      "outputs": [],
      "source": [
        "scale_labels = train_labels + validation_labels\n",
        "output_scaler = scaler.fit(np.array(scale_labels).reshape(len(scale_labels), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPU-yTjc9Lky"
      },
      "outputs": [],
      "source": [
        "input_X = array_stacker(train_data)\n",
        "input_y = output_scaler.transform(np.array(train_labels).reshape(-1, 1))\n",
        "\n",
        "validation_X = array_stacker(validation_data)\n",
        "validation_y = output_scaler.transform(np.array(validation_labels).reshape(-1, 1))\n",
        "\n",
        "predict_X = array_stacker(predict_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RJYiWvg4cKK"
      },
      "outputs": [],
      "source": [
        "input_X = tf.reshape(input_X, (input_X.shape[1], input_X.shape[2], input_X.shape[3]))\n",
        "input_y = tf.reshape(input_y, (input_y.shape[0], 1))\n",
        "validation_X = tf.reshape(validation_X, (validation_X.shape[1], validation_X.shape[2], validation_X.shape[3]))\n",
        "validation_y = tf.reshape(validation_y, (validation_y.shape[0], 1))\n",
        "predict_X = tf.reshape(predict_X, (predict_X.shape[1], predict_X.shape[2], predict_X.shape[3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qtasmi79Lw-"
      },
      "outputs": [],
      "source": [
        "model_3 = Sequential([\n",
        "        tf.keras.layers.LSTM(32, dropout=.25, activation='tanh', return_sequences=True),# return_sequences=True)),\n",
        "        tf.keras.layers.LSTM(16, dropout=.25, activation='tanh', return_sequences=True),\n",
        "        tf.keras.layers.LSTM(8),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model_3.compile(optimizer='Adam', loss='mse', metrics=[rmse])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmmXsZYpnhtN"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "epochs = 10000\n",
        "\n",
        "history_3 = model_3.fit(x=input_X,\n",
        "              y=input_y,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=2,\n",
        "              validation_data=(validation_X, validation_y),\n",
        "              callbacks=EarlyStopping(monitor='val_rmse', restore_best_weights=True, patience=150)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2RdzcMwnh8f"
      },
      "outputs": [],
      "source": [
        "plt.plot(history_3.history['rmse'])\n",
        "plt.plot(history_3.history['val_rmse'])\n",
        "plt.title('Model RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Extract the RMSE values from the history\n",
        "rmse_values3 = history_3.history['val_rmse']\n",
        "\n",
        "# Extract the final and lowest RMSE values\n",
        "print(\"Last: \", rmse_values3[-1], \"Best: \", min(rmse_values3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gDs_2C37ND8"
      },
      "outputs": [],
      "source": [
        "predictions = output_scaler.inverse_transform(model_3.predict(predict_X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5esXhYkfCi"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame()\n",
        "results['Ticker'] = frame_dict.keys()\n",
        "results['Most Recent Close'] = [close_dict[each] for each in results['Ticker']]\n",
        "results['Predicted Price'] = predictions\n",
        "results['Percent Change'] = ((results['Predicted Price'] - results['Most Recent Close']) / results['Most Recent Close']) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C55SWYwfPkEw"
      },
      "outputs": [],
      "source": [
        "print(results.sort_values(by='Percent Change', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOzFnX0YCp5G"
      },
      "outputs": [],
      "source": [
        "shortened_results = results.drop(results[results['Most Recent Close'] < 1.00].index)\n",
        "print(shortened_results.sort_values(by='Percent Change', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for saving shortened_results into Excel\n",
        "writer = pd.ExcelWriter('drive/My Drive/AI Predictions.xlsx', engine='openpyxl')\n",
        "tz = pytz.timezone('US/Eastern')\n",
        "today = datetime.now(tz).strftime('%Y-%m-%d')\n",
        "shortened_results.to_excel(writer, sheet_name=today, index=False)\n",
        "\n",
        "# Accessing the workbook and active sheet\n",
        "workbook  = writer.book\n",
        "worksheet = writer.sheets[today]\n",
        "\n",
        "# Setting column width based on the maximum length of each column header\n",
        "for i, column in enumerate(shortened_results.columns):\n",
        "    column_length = max(shortened_results[column].astype(str).map(len).max(), len(column))\n",
        "    worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = column_length + 2\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "AGz72rwxC4ab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}